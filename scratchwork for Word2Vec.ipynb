{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import brown\n",
    "b = Word2Vec(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_questions, generate_missing=False):\n",
    "    embeddings = clean_questions['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_word2vec_embeddings(word2vec, df)\n",
    "X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(embeddings, list_labels, \n",
    "                                                                                        test_size=0.2, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 16))          \n",
    "plot_LSA(embeddings, list_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_w2v = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n",
    "                         multi_class='multinomial', random_state=40)\n",
    "clf_w2v.fit(X_train_word2vec, y_train_word2vec)\n",
    "y_predicted_word2vec = clf_w2v.predict(X_test_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_word2vec, precision_word2vec, recall_word2vec, f1_word2vec = get_metrics(y_test_word2vec, y_predicted_word2vec)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_word2vec, precision_word2vec, \n",
    "                                                                       recall_word2vec, f1_word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_w2v = confusion_matrix(y_test_word2vec, y_predicted_word2vec)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "grade_categories = ('Preschool/Pre-K','K-2','3-5', '6-8', '9-12')\n",
    "plot = plot_confusion_matrix(cm_w2v, classes=grade_categories, normalize=True, title='Confusion matrix')\n",
    "plt.show()\n",
    "print(\"Word2Vec confusion matrix\")\n",
    "print(cm_w2v)\n",
    "print(\"TFIDF confusion matrix\")\n",
    "print(cm2)\n",
    "print(\"BoW confusion matrix\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for idy, row in enumerate(cm2):\n",
    "    for idx, col in enumerate(row):\n",
    "        res.append({'x': idx, 'y': idy, 'color': 'rbg(0, 0, {})'.format(col * 255 / 321)})\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "X_train_data, X_test_data, y_train_data, y_test_data = train_test_split(list_corpus, list_labels, test_size=0.2, \n",
    "                                                                                random_state=40)\n",
    "vector_store = word2vec\n",
    "def word2vec_pipeline(examples):\n",
    "    global vector_store\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized_list = []\n",
    "    for example in examples:\n",
    "        example_tokens = tokenizer.tokenize(example)\n",
    "        vectorized_example = get_average_word2vec(example_tokens, vector_store, generate_missing=False, k=300)\n",
    "        tokenized_list.append(vectorized_example)\n",
    "    return clf_w2v.predict_proba(tokenized_list)\n",
    "\n",
    "c = make_pipeline(count_vectorizer, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_one_instance(instance, class_names):\n",
    "    explainer = LimeTextExplainer(class_names=class_names)\n",
    "    exp = explainer.explain_instance(instance, word2vec_pipeline, num_features=6, labels=[0, 1, 2, 3, 4])\n",
    "    return exp\n",
    "\n",
    "def visualize_one_exp(features, labels, index, class_names = ('Preschool/Pre-K','K-2','3-5', '6-8', '9-12',)):\n",
    "    exp = explain_one_instance(features[index], class_names = class_names)\n",
    "    print('Index: %d' % index)\n",
    "    print('True class: %s' % class_names[labels[index]])\n",
    "    exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_one_exp(X_test_data, y_test_data, 63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_one_exp(X_test_data, y_test_data, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "random.seed(40)\n",
    "\n",
    "def get_statistical_explanation(test_set, sample_size, word2vec_pipeline, label_dict):\n",
    "    sample_sentences = random.sample(test_set, sample_size)\n",
    "    explainer = LimeTextExplainer()\n",
    "    \n",
    "    labels_to_sentences = defaultdict(list)\n",
    "    contributors = defaultdict(dict)\n",
    "    \n",
    "    # First, find contributing words to each class\n",
    "    for sentence in sample_sentences:\n",
    "        probabilities = word2vec_pipeline([sentence])\n",
    "        curr_label = probabilities[0].argmax()\n",
    "        labels_to_sentences[curr_label].append(sentence)\n",
    "        exp = explainer.explain_instance(sentence, word2vec_pipeline, num_features=6, labels=[curr_label])\n",
    "        listed_explanation = exp.as_list(label=curr_label)\n",
    "        \n",
    "        for word,contributing_weight in listed_explanation:\n",
    "            if word in contributors[curr_label]:\n",
    "                contributors[curr_label][word].append(contributing_weight)\n",
    "            else:\n",
    "                contributors[curr_label][word] = [contributing_weight]    \n",
    "    \n",
    "    # average each word's contribution to a class, and sort them by impact\n",
    "    average_contributions = {}\n",
    "    sorted_contributions = {}\n",
    "    for label,lexica in contributors.items():\n",
    "        curr_label = label\n",
    "        curr_lexica = lexica\n",
    "        average_contributions[curr_label] = pd.Series(index=curr_lexica.keys())\n",
    "        for word,scores in curr_lexica.items():\n",
    "            average_contributions[curr_label].loc[word] = np.sum(np.array(scores))/sample_size\n",
    "        detractors = average_contributions[curr_label].sort_values()\n",
    "        supporters = average_contributions[curr_label].sort_values(ascending=False)\n",
    "        sorted_contributions[label_dict[curr_label]] = {\n",
    "            'detractors': detractors,\n",
    "            'supporters': supporters\n",
    "        }\n",
    "    return sorted_contributions\n",
    "\n",
    "label_to_text = {\n",
    "    0: 'Preschool/Pre-K',\n",
    "    1: 'K-2',\n",
    "    2: '3-5',\n",
    "    3: '6-8',\n",
    "    4: '9-12',\n",
    "}\n",
    "sorted_contributions = get_statistical_explanation(X_test_data, 5, word2vec_pipeline, label_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_important_words_w2v(top_scores, top_words, bottom_scores, bottom_words, name, grade_category):\n",
    "    y_pos = np.arange(len(top_words))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))          \n",
    "    plt.subplot(121)\n",
    "    plt.barh(y_pos,bottom_scores, align='center', alpha=0.5)\n",
    "    plt.title('Irrelevant to\\n{}'.format(grade_category))\n",
    "    plt.yticks(y_pos, bottom_words)\n",
    "    plt.suptitle('Key words', fontsize=16)\n",
    "    plt.xlabel('Importance')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.barh(y_pos,top_scores, align='center', alpha=0.5)\n",
    "    plt.title('Relevant to\\n{}'.format(grade_category))\n",
    "    plt.yticks(y_pos, top_words)\n",
    "    plt.suptitle(name, fontsize=16)\n",
    "    plt.xlabel('Importance')\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.8)\n",
    "    plt.show()\n",
    "\n",
    "grade_category = '9-12'\n",
    "top_words = sorted_contributions[grade_category]['supporters'][:10].index.tolist()\n",
    "top_scores = sorted_contributions[grade_category]['supporters'][:10].tolist()\n",
    "bottom_words = sorted_contributions[grade_category]['detractors'][:10].index.tolist()\n",
    "bottom_scores = sorted_contributions[grade_category]['detractors'][:10].tolist()\n",
    "\n",
    "plot_important_words_w2v(top_scores,\n",
    "                         top_words,\n",
    "                         bottom_scores,\n",
    "                         bottom_words,\n",
    "                         \"Most important words for relevance\",\n",
    "                         grade_category=grade_category)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
